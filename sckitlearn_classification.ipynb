{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "199d9e1b-ab98-423e-81f6-de6cf1f4be5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\motoc\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (1.1.3)\n",
      "Collecting janome\n",
      "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
      "     --------------------------------------- 19.7/19.7 MB 18.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\motoc\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\motoc\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\motoc\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn) (1.23.2)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\motoc\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Installing collected packages: janome\n",
      "Successfully installed janome-0.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn janome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97a50f1a-6a1e-4efc-80ae-a5af519ec166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pdb\n",
    "import requests\n",
    "import json\n",
    "\n",
    "static_path=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4aa80e34-cee2-463b-a171-e72c5db5bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"test_gpu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "851213ef-9ea2-4b44-960e-92230033b10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>group</th>\n",
       "      <th>querystr</th>\n",
       "      <th>ai_train_label</th>\n",
       "      <th>ai_predict_label</th>\n",
       "      <th>ai_predict_score</th>\n",
       "      <th>ai_predict_label_candidate</th>\n",
       "      <th>ai_predict_score_candidate</th>\n",
       "      <th>ai_predict_candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fc1de8</td>\n",
       "      <td>36</td>\n",
       "      <td>WO2022014496</td>\n",
       "      <td>RF-D/SAW</td>\n",
       "      <td>RF-D/SAW</td>\n",
       "      <td>5.666085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6505c2</td>\n",
       "      <td>36</td>\n",
       "      <td>WO2022014497</td>\n",
       "      <td>RF-D/SAW</td>\n",
       "      <td>RF-D/SAW</td>\n",
       "      <td>5.595006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72f8c1</td>\n",
       "      <td>36</td>\n",
       "      <td>JPWO2018154950</td>\n",
       "      <td>RF-D/SAW</td>\n",
       "      <td>RF-D/SAW</td>\n",
       "      <td>6.229929</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1566c2</td>\n",
       "      <td>36</td>\n",
       "      <td>WO2022173038</td>\n",
       "      <td>RF-D/SAW</td>\n",
       "      <td>RF-D/SAW</td>\n",
       "      <td>5.351918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97e19e</td>\n",
       "      <td>36</td>\n",
       "      <td>JP2021190846</td>\n",
       "      <td>RF-D/SAW</td>\n",
       "      <td>RF-D/SAW</td>\n",
       "      <td>5.419605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4967</th>\n",
       "      <td>3aba53</td>\n",
       "      <td>1</td>\n",
       "      <td>JP2013107344</td>\n",
       "      <td>OTHERS/OTHERX</td>\n",
       "      <td>RF-D/CMFD</td>\n",
       "      <td>0.567510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4968</th>\n",
       "      <td>b5ab78</td>\n",
       "      <td>33</td>\n",
       "      <td>JPWO2007029495</td>\n",
       "      <td>NEWPRD/EOLINC</td>\n",
       "      <td>RF-D/RF-FEM</td>\n",
       "      <td>0.692119</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4969</th>\n",
       "      <td>d6e4d2</td>\n",
       "      <td>19</td>\n",
       "      <td>JP2010021411</td>\n",
       "      <td>CAP/HI-CAP</td>\n",
       "      <td>CAP/HI-CAP</td>\n",
       "      <td>0.619054</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>0a0fab</td>\n",
       "      <td>10</td>\n",
       "      <td>JPWO2007007511</td>\n",
       "      <td>EMC/EMIFIL</td>\n",
       "      <td>FD/AD</td>\n",
       "      <td>0.627821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>9064ed</td>\n",
       "      <td>19</td>\n",
       "      <td>JP2009047493</td>\n",
       "      <td>CAP/HI-CAP</td>\n",
       "      <td>HELPRD/HELNEW</td>\n",
       "      <td>0.543375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4972 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  group        querystr ai_train_label ai_predict_label  \\\n",
       "0     fc1de8     36    WO2022014496       RF-D/SAW         RF-D/SAW   \n",
       "1     6505c2     36    WO2022014497       RF-D/SAW         RF-D/SAW   \n",
       "2     72f8c1     36  JPWO2018154950       RF-D/SAW         RF-D/SAW   \n",
       "3     1566c2     36    WO2022173038       RF-D/SAW         RF-D/SAW   \n",
       "4     97e19e     36    JP2021190846       RF-D/SAW         RF-D/SAW   \n",
       "...      ...    ...             ...            ...              ...   \n",
       "4967  3aba53      1    JP2013107344  OTHERS/OTHERX        RF-D/CMFD   \n",
       "4968  b5ab78     33  JPWO2007029495  NEWPRD/EOLINC      RF-D/RF-FEM   \n",
       "4969  d6e4d2     19    JP2010021411     CAP/HI-CAP       CAP/HI-CAP   \n",
       "4970  0a0fab     10  JPWO2007007511     EMC/EMIFIL            FD/AD   \n",
       "4971  9064ed     19    JP2009047493     CAP/HI-CAP    HELPRD/HELNEW   \n",
       "\n",
       "      ai_predict_score  ai_predict_label_candidate  \\\n",
       "0             5.666085                         NaN   \n",
       "1             5.595006                         NaN   \n",
       "2             6.229929                         NaN   \n",
       "3             5.351918                         NaN   \n",
       "4             5.419605                         NaN   \n",
       "...                ...                         ...   \n",
       "4967          0.567510                         NaN   \n",
       "4968          0.692119                         NaN   \n",
       "4969          0.619054                         NaN   \n",
       "4970          0.627821                         NaN   \n",
       "4971          0.543375                         NaN   \n",
       "\n",
       "      ai_predict_score_candidate  ai_predict_candidate  \n",
       "0                            NaN                   NaN  \n",
       "1                            NaN                   NaN  \n",
       "2                            NaN                   NaN  \n",
       "3                            NaN                   NaN  \n",
       "4                            NaN                   NaN  \n",
       "...                          ...                   ...  \n",
       "4967                         NaN                   NaN  \n",
       "4968                         NaN                   NaN  \n",
       "4969                         NaN                   NaN  \n",
       "4970                         NaN                   NaN  \n",
       "4971                         NaN                   NaN  \n",
       "\n",
       "[4972 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4c047f0-baf7-4774-b185-26a926b7e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "import numpy as np\n",
    "import pdb\n",
    "from typing import Tuple\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from time import time\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "from collections import defaultdict\n",
    "import codecs\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from janome.tokenfilter import POSKeepFilter\n",
    "\n",
    "\n",
    "def tfidf(df,head) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \n",
    "    t0 = time()\n",
    "\n",
    "    patentid_list=[] #特許のID配列\n",
    "    tfidf_word=[] #TFIDFの語句配列\n",
    "    tfidf_score=[] #TFIDFのスコア配列\n",
    "    train_label=[]\n",
    "\n",
    "    for id,tfidf in zip(df.patent_id,df.tfidf):\n",
    "        df1 = pd.DataFrame(list(tfidf.items()),columns= ['word','tfidf'])\n",
    "        df1['tfidf']= df1['tfidf'].astype(float)\n",
    "       \n",
    "        #df1 = df1[df1['tfidf']> head ]\n",
    "        df1 = df1.sort_values(by='tfidf',ascending=False).head(head)\n",
    "        patentid_list.append(id)\n",
    "        tfidf_word.append(df1['word'].tolist())\n",
    "        tfidf_score.append(df1['tfidf'].tolist())\n",
    "    \n",
    "    train_label = df['ai_train_label'].tolist()\n",
    "\n",
    "    unique_word = set([x for row in tfidf_word for x in row])\n",
    "    unique_idx = {w:i for i,w in enumerate(unique_word)}\n",
    "\n",
    "    print('number of unique words = {}'.format(len(unique_word)))\n",
    "    print('number of data lengths = {}'.format(len(tfidf_word)))\n",
    "\n",
    "    to_unique_idx = np.vectorize(unique_idx.get)\n",
    "    tfidf_arr = np.zeros((len(tfidf_word),len(unique_word)), dtype='float32')\n",
    "\n",
    "    for i,(words,scores) in enumerate(zip(tfidf_word,tfidf_score)):\n",
    "        words = np.array(words)\n",
    "        scores = np.array(scores,dtype=float)\n",
    "    \n",
    "        try:\n",
    "            idx = to_unique_idx(words)\n",
    "            tfidf_arr[i,idx] = scores\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# 変換前のデータ\n",
    "    train_arr = np.array(train_label)\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_arr)\n",
    "    label_names = le.classes_\n",
    "\n",
    "    waist_time = time() - t0\n",
    "    print(\"tfidf time: %0.3fs\" % waist_time)\n",
    "\n",
    "    unique_words = np.array(list(unique_word))\n",
    "\n",
    "    return tfidf_arr, train_arr, label_names , unique_words\n",
    "\n",
    "\n",
    "def get_token(text):\n",
    "    t = Tokenizer()\n",
    "    tokens = t.tokenize(text)\n",
    "    \n",
    "    word = \"\"\n",
    "    for token in tokens:\n",
    "        part_of_speech = token.part_of_speech.split(\",\")[0]\n",
    "        if part_of_speech == \"名詞\":\n",
    "            word +=token.surface + \" \"\n",
    "        if part_of_speech == \"動詞\":\n",
    "            word +=token.base_form+ \" \"\n",
    "        if part_of_speech == \"形容詞\":\n",
    "            word +=token.base_form+ \" \"\n",
    "        if part_of_speech == \"形容動詞\":\n",
    "            word +=token.base_form+ \" \"\n",
    "    return word\n",
    "\n",
    "\n",
    "def tfidf_calc(target,token,max_word):\n",
    "    corpus=[]\n",
    "    \n",
    "    for item in tqdm(target): \n",
    "        token=get_token(item)\n",
    "        corpus.append(token)\n",
    "\n",
    "    vectorize = TfidfVectorizer(use_idf=True,max_df=0.9)\n",
    "    \n",
    "    tfidf = vectorize.fit_transform(corpus)\n",
    "    \n",
    "    output =[]\n",
    "    words = vectorize.get_feature_names()\n",
    "    for doc_id,vec in zip(range(len(corpus)),tfidf.toarray()):\n",
    "        jstring=\"\"\n",
    "        for w_id,tfidf in sorted(enumerate(vec),key=lambda x:x[1],reverse=True)[:max_word]:\n",
    "            lemma= words[w_id]\n",
    "            jstring += '\"{}\"=>\"{:.7g}\",'.format(lemma,tfidf*100)\n",
    "        output.append(jstring)\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73d09cfb-5fe2-4b52-b4c8-6f1cd2a8706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_patent_list(body):\n",
    "    if body:\n",
    "        data = body['patent']\n",
    "        tfidf_max_length= body['tfidf_max_length']\n",
    "        #numofword = body['numofword']\n",
    "        df = pd.read_json(data)##CrossValidationで使うためにデータフレームにする\n",
    "        patent_list = df['querystr'].tolist()##クエリストリングをリストにする\n",
    "\n",
    "        item_patent = cruds.search_patents(\n",
    "            db=db, mode=mode, patent_numbers=patent_list)\n",
    "        # modelスケマからjson→pandasに変換する\n",
    "        json_item_data = jsonable_encoder(item_patent)\n",
    "        df_s = pd.DataFrame(json_item_data)\n",
    "        \n",
    "        for index,row in df_s.iterrows():\n",
    "            label = df[df.querystr == row.koukai_number].ai_train_label\n",
    "            df_s.at[index,'ai_train_label'] = label.values[0]\n",
    "            \n",
    "        # pickleにつける\n",
    "        df_s.to_pickle(static_path + body['token'])\n",
    "\n",
    "       # リダイレクト\n",
    "        return \"/train_tfidf/?token={}&tfidf_max_length={}&numofword={}\".format(body['token'], tfidf_max_length,numofword)\n",
    "\n",
    "staric_path= \"\"\n",
    "    \n",
    "def post_train_tfidf(token, tfidf_max_length):\n",
    "    unpickled_df = pd.read_pickle(static_path + token)\n",
    "    tfidf_arr, label_arr,label_names ,unique_word= tfidf(unpickled_df, tfidf_max_length)\n",
    "    # pickleにつける\n",
    "\n",
    "    np.save(static_path +'tfidf_arr.npy', tfidf_arr)\n",
    "    np.save(static_path +'label_arr.npy', label_arr)\n",
    "    np.save(static_path +'label_names.npy', label_names)\n",
    "    np.save(static_path + 'unique_names.npy', unique_word)\n",
    "    # リダイレクト\n",
    "    return {\"token\": token, \"tfidf_arr\": tfidf_arr.shape, \"label_arr\": label_arr.shape, \"label_names\": label_names.shape, \"unique_word\": unique_word.shape}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ada71367-51f2-417d-a688-b99e9ea4bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#絞り込み後の特許リストで交差検証を実施するルーチン\n",
    "def mlFlow(df,token,tfidf_target,num):\n",
    "    url =\"http://172.26.106.79:7999/patent_list2\"\n",
    "#JSON形式のデータ\n",
    "    json_data = {\n",
    "    'patent': df.to_json(),\n",
    "    'token':token,\n",
    "    'tfidf_max_length':tfidf_target\n",
    "    }    \n",
    "    \n",
    "    with open('patent_number.json', 'w') as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    res = requests.post( url,  json = json_data)   #dataを指定する    \n",
    "    json_item_data = res.json()\n",
    "    \n",
    "    return json_item_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22495a86-2b9d-43f0-a7e2-19fa4da86834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d4a2418-e8fe-4e5f-984f-f9c11ac2e0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_data = mlFlow(df,\"demo\",200,5) #patent number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ac98c28-cfa1-468a-a1c3-09667caff4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('patent_number.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "df_s = pd.DataFrame(json_data)\n",
    "\n",
    "for index,row in df_s.iterrows():\n",
    "    label = df[df.querystr == row.koukai_number].ai_train_label\n",
    "    df_s.at[index,'ai_train_label'] = label.values[0]\n",
    "            \n",
    "        # pickleにつける\n",
    "df_s.to_pickle(static_path + \"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9fb34bc-16e0-4264-bd9c-52a5960e2b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea890719-5a61-4550-b541-b142000de48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words = 58436\n",
      "number of data lengths = 4972\n",
      "tfidf time: 4.917s\n"
     ]
    }
   ],
   "source": [
    "result = post_train_tfidf(\"demo\",200) #tfidf dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a316ab3-37f2-4737-8550-3fd31569ec6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in c:\\users\\motoc\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (2.2.2)\n",
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-1.4.43-cp38-cp38-win_amd64.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 9.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: itsdangerous>=2.0 in c:\\users\\motoc\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from flask) (2.1.2)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in c:\\users\\motoc\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from flask) (2.2.2)\n",
      "Requirement already satisfied: click>=8.0 in c:\\users\\motoc\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from flask) (8.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\motoc\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from flask) (3.1.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in c:\\users\\motoc\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from flask) (4.11.4)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-2.0.1-cp38-cp38-win_amd64.whl (190 kB)\n",
      "     -------------------------------------- 190.7/190.7 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\motoc\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from click>=8.0->flask) (0.4.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\motoc\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from importlib-metadata>=3.6.0->flask) (3.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\motoc\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from Jinja2>=3.0->flask) (2.1.1)\n",
      "Installing collected packages: greenlet, sqlalchemy\n",
      "Successfully installed greenlet-2.0.1 sqlalchemy-1.4.43\n"
     ]
    }
   ],
   "source": [
    "!pip install flask sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0507ed8-72f5-4821-bf5d-807b19675364",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split,LeaveOneOut, cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.linear_model import Perceptron\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from urllib3 import HTTPResponse\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "from fastapi.responses import JSONResponse\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from fastapi import APIRouter, Depends, File, HTTPException, Query, UploadFile\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "from fastapi.responses import StreamingResponse\n",
    "from flask import jsonify\n",
    "from sklearn.feature_extraction import img_to_graph\n",
    "from sqlalchemy.orm import Session\n",
    "import requests\n",
    "from fastapi.responses import RedirectResponse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9774989-182c-42c5-959b-bb45f37b44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlflow(\n",
    "            token: str = \"\",numofword: int = 100,\n",
    "            cvmode: str =\"KFold\",\n",
    "            nsplit: int = 5,\n",
    "            n_candidate: int = 5\n",
    "            ):\n",
    "\n",
    "    \n",
    "    tfidf_arr=np.load(static_path +'tfidf_arr.npy')\n",
    "    label_arr=np.load(static_path +'label_arr.npy')\n",
    "    label_names=np.load(static_path +'label_names.npy')\n",
    "    unique_word = np.load(static_path +'unique_names.npy')\n",
    "\n",
    "    if cvmode == \"KFold\":# kfoldでクロスバリデーションを行う\n",
    "        CrossVaild = StratifiedKFold(n_splits=nsplit, shuffle=True)#,random_state=42)\n",
    "    elif cvmode == \"LeaveOneOut\":# 1 vs 1 でクロスバリデーションを行う\n",
    "        CrossVaild = LeaveOneOut()\n",
    "\n",
    "    #predict-proba が使えるモデルを使う\n",
    "    clf =  LogisticRegression(class_weight='balanced',max_iter=300)\n",
    "\n",
    "    ext = []\n",
    "    mlresult =[]\n",
    "    mlresult2=[]\n",
    "\n",
    "    for train_index, test_index in tqdm(CrossVaild.split(tfidf_arr, label_arr)):\n",
    "        x_train, x_test = tfidf_arr[train_index], tfidf_arr[test_index]\n",
    "        y_train, y_test = label_arr[train_index], label_arr[test_index]\n",
    "\n",
    "    #ms = StandardScaler()\n",
    "        ms = MinMaxScaler()\n",
    "        X_train = ms.fit_transform(x_train)\n",
    "        X_test = ms.transform(x_test)\n",
    "\n",
    "        clf,ostr = benchmark(clf,X_train, X_test, y_train, y_test)\n",
    "\n",
    "#def print_top10(feature_names, clf, class_labels):\n",
    "        if hasattr(clf, \"coef_\"):\n",
    "            #ostr[\"dimensionality\"] = \"%d\" % clf.coef_.shape[1] \n",
    "            #ostr[\"density\"] = \"%f\" % density(clf.coef_) \n",
    "            ostr[\"top10\"] = print_top10(numofword,unique_word, clf, label_names)\n",
    "            intercept = clf.intercept_.tolist()\n",
    "            mlresult.append(ostr)\n",
    "            mlresult2.append( dict(zip(label_names,intercept))) #mlresult.append(ostr)\n",
    "        \n",
    "        pred_proba_train = clf.predict_proba(X_train)\n",
    "        pred_proba_test = clf.predict_proba(X_test)\n",
    "\n",
    "        df = pd.read_pickle(static_path + token)\n",
    " \n",
    "        labels = set()   # 空のセットを生成(set([]))\n",
    "        for i,value in enumerate(test_index):\n",
    "            labels.add( df.loc[value].ai_train_label)\n",
    "        \n",
    "        labels = list(labels) #listに戻す\n",
    "\n",
    "        #pdb.set_trace()\n",
    "\n",
    "        for ind, value in enumerate(test_index):\n",
    "            output = {\"id\": str(uuid.uuid4())[:6]}\n",
    "            output.update({\"group\": len(mlresult)-1})  #labels.index( df.loc[value].ai_train_label)})\n",
    "            num = df.loc[value].koukai_number\n",
    "            output.update({\"querystr\": num})\n",
    "            output.update({\"ai_train_label\": df.loc[value].ai_train_label})\n",
    "            output.update(\n",
    "            {\"ai_predict_label(pf)\": df.loc[value].ai_predict_label})\n",
    "            output.update(\n",
    "            {\"ai_predict_score(pf)\": df.loc[value].ai_predict_score})\n",
    "            \n",
    "            ds = {key: val for key, val in zip(label_names, map(\n",
    "            '{:f}'.format, pred_proba_test[ind].tolist()))}\n",
    "            #pdb.set_trace()\n",
    "            max_kv = max(ds.items(), key=lambda x: x[1])\n",
    "            output.update({\"ai_predict_label\": max_kv[0]})\n",
    "            output.update({\"ai_predict_score\": max_kv[1]})\n",
    "            ds_sel = dict( sorted(ds.items(),key=lambda x:x[1],reverse=True)[0:n_candidate])\n",
    "            output.update({\"ai_predict_candidate\": json.dumps(ds_sel)})\n",
    "            \n",
    "            #pdb.set_trace()\n",
    "            \n",
    "            ext.append(output)\n",
    "\n",
    "    return {\"crossvalid\":ext,\"mlresult\":mlresult,\"mlresult2\":mlresult2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c04b808-4f15-4bdf-9797-a49024c5a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlFlow2(token,num_word,cvmode,nsplit,n_candidate):\n",
    "    url = 'http://172.26.106.79:7999/calc_crossvalid/'\n",
    "        \n",
    "    if cvmode==\"LeaveOneOut\":\n",
    "        nsplit=2\n",
    "        \n",
    "    payload  ={\n",
    "     'token':token,\n",
    "     'numofword':num_word,\n",
    "     'cvmode':cvmode,\n",
    "     'nsplit':nsplit,\n",
    "     'n_candidate':n_candidate\n",
    "    }\n",
    "\n",
    "   #get送信\n",
    "    \n",
    "    res = get_mlflow(token, num_word, cvmode, nsplit , n_candidate)  #requests.get( url,  params=payload)   #dataを指定する    )\n",
    "\n",
    "    json_item_data = res\n",
    "        \n",
    "    #戻り値はJSON形式　dataframeと、mlresultの２つのデータ保存する\n",
    "    df=json_item_data[\"crossvalid\"]\n",
    "    mlresult = json_item_data[\"mlresult\"]\n",
    "    mlresult2 = json_item_data[\"mlresult2\"]\n",
    "\n",
    "    cust_df = pd.json_normalize(df)\n",
    "    stjson = cust_df.to_dict(\"records\") \n",
    "\n",
    "    return stjson,mlresult,mlresult2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "524149fa-65b3-4bfe-9a68-81c52e115b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "import pdb\n",
    "from typing import Tuple\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import uuid\n",
    "\n",
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\"\n",
    "\n",
    "#print_top10(unique_word, clf, label_names)\n",
    "def print_top10(topN,feature_names, clf, class_labels):\n",
    "    \n",
    "    \n",
    "    restr = {}\n",
    "\n",
    "    if len(class_labels)>2:\n",
    "        N= -1 * topN\n",
    "        for i, class_label in enumerate(class_labels):\n",
    "            \n",
    "            top10 = np.argsort(clf.coef_[i])[N:]\n",
    "            top10 = top10[::-1]\n",
    "            top10_vec = np.sort(clf.coef_[i])[N:]\n",
    "            top10_vec = top10_vec[::-1]\n",
    "            dic = {key: val for key, val in zip(feature_names[top10],top10_vec)}\n",
    "            restr[class_label] = dic\n",
    "            \n",
    "            #print(\"{} {} {}\".format( class_label,len(top10),len(top10_vec)))\n",
    "\n",
    "        return restr\n",
    "    else:\n",
    "        N= -1 * topN\n",
    "        \n",
    "        top10 = np.argsort(clf.coef_[0])[N:]\n",
    "        top10 = top10[::-1]\n",
    "        top10_vec = np.sort(clf.coef_[0])[N:]\n",
    "        top10_vec = top10_vec[::-1]\n",
    "        dic = {key: val for key, val in zip(feature_names[top10],top10_vec)}\n",
    "        restr[class_labels[1]] = dic\n",
    "        #print(\"{} {} {}\".format( class_labels[0],len(top10),len(top10_vec)))\n",
    "\n",
    "        top10 = np.argsort(clf.coef_[0])[:topN]\n",
    "        top10_vec = np.sort(clf.coef_[0])[:topN]\n",
    "        dic = {key: val * -1 for key, val in zip(feature_names[top10],top10_vec)}\n",
    "        restr[class_labels[0]] = dic\n",
    "        #print(\"{} {} {}\".format( class_labels[1],len(top10),len(top10_vec)))\n",
    "\n",
    "        return restr\n",
    "\n",
    "def benchmark(clf,X_train, X_test, y_train, y_test):\n",
    "    outstr = {}\n",
    "\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_time = time() - t0\n",
    "    outstr[\"train time\"] = \"%0.3f\" % train_time \n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "\n",
    "    #print(confusion_matrix(y_test, pred))\n",
    "\n",
    "    test_time = time() - t0\n",
    "    outstr[\"test time\"] = \"%0.3f\" % test_time \n",
    "    \n",
    "#正解率\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    outstr[\"accuracy\"] = \"%0.3f\" % score \n",
    "\n",
    "#再現性\n",
    "    recall = metrics.recall_score(y_test, pred, average='macro')\n",
    "    outstr[\"recall\"] = \"%0.3f\" % recall\n",
    "\n",
    "    return clf,outstr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f6e8b6-07f7-4ec9-b475-ef71b5d4615b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [06:30, 99.04s/it]"
     ]
    }
   ],
   "source": [
    "crossvalid, result, result2 = mlFlow2(token=\"demo\",num_word=5,cvmode=\"KFold\",nsplit = 5,n_candidate = 5) #token,num_word,cvmode,nsplit,n_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47156bf1-dcf8-4d89-b329-70104313789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[result[x]['train time'] for x in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ee18d3-b965-443b-937f-b3e6c7c3eaaa",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6ce784bd-6dc8-4a84-b7d8-e42e7fb53169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (1.50.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (1.22.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (22.10.26)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (4.3.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (59.8.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.14.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from packaging->tensorflow) (3.0.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\mm05162\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "80ebaef9-d945-4f9e-8c73-e6b0d9d85e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def classification_model(output_dim,input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(output_dim, input_shape=(input_dim,), activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def benchmark_keras(clf,X_train, X_test, y_train, y_test,epochs=500, verbose=0):\n",
    "    outstr = {}\n",
    "\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train,epochs, verbose)\n",
    "    \n",
    "    pdb.set_trace()\n",
    "    \n",
    "    train_time = time() - t0\n",
    "    outstr[\"train time\"] = \"%0.3f\" % train_time \n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "\n",
    "    #print(confusion_matrix(y_test, pred))\n",
    "\n",
    "    test_time = time() - t0\n",
    "    outstr[\"test time\"] = \"%0.3f\" % test_time \n",
    "    \n",
    "#正解率\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    outstr[\"accuracy\"] = \"%0.3f\" % score \n",
    "\n",
    "#再現性\n",
    "    recall = metrics.recall_score(y_test, pred, average='macro')\n",
    "    outstr[\"recall\"] = \"%0.3f\" % recall\n",
    "\n",
    "    return clf,outstr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eaa35425-8e6f-447d-8349-fb059324ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlflow_gpu(\n",
    "            token: str = \"\",numofword: int = 100,\n",
    "            cvmode: str =\"KFold\",\n",
    "            nsplit: int = 5,\n",
    "            n_candidate: int = 5\n",
    "            ):\n",
    "\n",
    "    tfidf_arr=np.load(static_path +'tfidf_arr.npy').astype('float32')\n",
    "    label_arr=np.load(static_path +'label_arr.npy').astype('float32')\n",
    "    label_names=np.load(static_path +'label_names.npy').astype('float32')\n",
    "    unique_word = np.load(static_path +'unique_names.npy').astype('float32')\n",
    "\n",
    "    if cvmode == \"KFold\":# kfoldでクロスバリデーションを行う\n",
    "        CrossVaild = StratifiedKFold(n_splits=nsplit, shuffle=True)#,random_state=42)\n",
    "    elif cvmode == \"LeaveOneOut\":# 1 vs 1 でクロスバリデーションを行う\n",
    "        CrossVaild = LeaveOneOut()\n",
    "\n",
    "    #predict-proba が使えるモデルを使う\n",
    "    \n",
    "    #training\n",
    "    clf = classification_model(\n",
    "input_dim = len(tfidf_arr),\n",
    "output_dim = len(label_names)# 3 possible outputs\n",
    "    )\n",
    "    #keras_model.fit(X_train, Y_train, epochs=500, verbose=0)\n",
    "    #clf =  LogisticRegression(class_weight='balanced',max_iter=300)\n",
    "\n",
    "    ext = []\n",
    "    mlresult =[]\n",
    "    mlresult2=[]\n",
    "\n",
    "\n",
    "    for train_index, test_index in tqdm(CrossVaild.split(tfidf_arr, label_arr)):\n",
    "        x_train, x_test = tfidf_arr[train_index], tfidf_arr[test_index]\n",
    "        y_train, y_test = label_arr[train_index], label_arr[test_index]\n",
    "\n",
    "    #ms = StandardScaler()\n",
    "        ms = MinMaxScaler()\n",
    "        X_train = ms.fit_transform(x_train)\n",
    "        X_test = ms.transform(x_test)\n",
    "\n",
    "        clf,ostr = benchmark_keras(clf,X_train, X_test, y_train, y_test, epochs=500, verbose=0)\n",
    "\n",
    "#def print_top10(feature_names, clf, class_labels):\n",
    "        if hasattr(clf, \"coef_\"):\n",
    "            #ostr[\"dimensionality\"] = \"%d\" % clf.coef_.shape[1] \n",
    "            #ostr[\"density\"] = \"%f\" % density(clf.coef_) \n",
    "            ostr[\"top10\"] = print_top10(numofword,unique_word, clf, label_names)\n",
    "            intercept = clf.intercept_.tolist()\n",
    "            mlresult.append(ostr)\n",
    "            mlresult2.append( dict(zip(label_names,intercept))) #mlresult.append(ostr)\n",
    "        \n",
    "        pred_proba_train = clf.predict_proba(X_train)\n",
    "        pred_proba_test = clf.predict_proba(X_test)\n",
    "\n",
    "        df = pd.read_pickle(static_path + token)\n",
    " \n",
    "        labels = set()   # 空のセットを生成(set([]))\n",
    "        for i,value in enumerate(test_index):\n",
    "            labels.add( df.loc[value].ai_train_label)\n",
    "        \n",
    "        labels = list(labels) #listに戻す\n",
    "\n",
    "        #pdb.set_trace()\n",
    "\n",
    "        for ind, value in enumerate(test_index):\n",
    "            output = {\"id\": str(uuid.uuid4())[:6]}\n",
    "            output.update({\"group\": len(mlresult)-1})  #labels.index( df.loc[value].ai_train_label)})\n",
    "            num = df.loc[value].koukai_number\n",
    "            output.update({\"querystr\": num})\n",
    "            output.update({\"ai_train_label\": df.loc[value].ai_train_label})\n",
    "            output.update(\n",
    "            {\"ai_predict_label(pf)\": df.loc[value].ai_predict_label})\n",
    "            output.update(\n",
    "            {\"ai_predict_score(pf)\": df.loc[value].ai_predict_score})\n",
    "            \n",
    "            ds = {key: val for key, val in zip(label_names, map(\n",
    "            '{:f}'.format, pred_proba_test[ind].tolist()))}\n",
    "            #pdb.set_trace()\n",
    "            max_kv = max(ds.items(), key=lambda x: x[1])\n",
    "            output.update({\"ai_predict_label\": max_kv[0]})\n",
    "            output.update({\"ai_predict_score\": max_kv[1]})\n",
    "            ds_sel = dict( sorted(ds.items(),key=lambda x:x[1],reverse=True)[0:n_candidate])\n",
    "            output.update({\"ai_predict_candidate\": json.dumps(ds_sel)})\n",
    "            \n",
    "            #pdb.set_trace()\n",
    "            \n",
    "            ext.append(output)\n",
    "\n",
    "    return {\"crossvalid\":ext,\"mlresult\":mlresult,\"mlresult2\":mlresult2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "15238ef5-775e-49c3-b587-1916c6ebeba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlFlow3(token,num_word,cvmode,nsplit,n_candidate):\n",
    "    url = 'http://172.26.106.79:7999/calc_crossvalid/'\n",
    "        \n",
    "    if cvmode==\"LeaveOneOut\":\n",
    "        nsplit=2\n",
    "        \n",
    "    payload  ={\n",
    "     'token':token,\n",
    "     'numofword':num_word,\n",
    "     'cvmode':cvmode,\n",
    "     'nsplit':nsplit,\n",
    "     'n_candidate':n_candidate\n",
    "    }\n",
    "\n",
    "   #get送信\n",
    "    \n",
    "    res = get_mlflow_gpu(token, num_word, cvmode, nsplit , n_candidate)  #requests.get( url,  params=payload)   #dataを指定する    )\n",
    "\n",
    "    json_item_data = res\n",
    "        \n",
    "    #戻り値はJSON形式　dataframeと、mlresultの２つのデータ保存する\n",
    "    df=json_item_data[\"crossvalid\"]\n",
    "    mlresult = json_item_data[\"mlresult\"]\n",
    "    mlresult2 = json_item_data[\"mlresult2\"]\n",
    "\n",
    "    cust_df = pd.json_normalize(df)\n",
    "    stjson = cust_df.to_dict(\"records\") \n",
    "\n",
    "    return stjson,mlresult,mlresult2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d5ebc468-8fd3-4d0a-b1ab-5d3920384553",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.16 GiB for an array with shape (290543792,) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [95]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m crossvalid, result, result2 \u001b[38;5;241m=\u001b[39m \u001b[43mmlFlow3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdemo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcvmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKFold\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnsplit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mn_candidate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [94]\u001b[0m, in \u001b[0;36mmlFlow3\u001b[1;34m(token, num_word, cvmode, nsplit, n_candidate)\u001b[0m\n\u001b[0;32m      7\u001b[0m  payload  \u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m      8\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m:token,\n\u001b[0;32m      9\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumofword\u001b[39m\u001b[38;5;124m'\u001b[39m:num_word,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_candidate\u001b[39m\u001b[38;5;124m'\u001b[39m:n_candidate\n\u001b[0;32m     13\u001b[0m  }\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#get送信\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m  res \u001b[38;5;241m=\u001b[39m \u001b[43mget_mlflow_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcvmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnsplit\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidate\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#requests.get( url,  params=payload)   #dataを指定する    )\u001b[39;00m\n\u001b[0;32m     19\u001b[0m  json_item_data \u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m     21\u001b[0m  \u001b[38;5;66;03m#戻り値はJSON形式　dataframeと、mlresultの２つのデータ保存する\u001b[39;00m\n",
      "Input \u001b[1;32mIn [93]\u001b[0m, in \u001b[0;36mget_mlflow_gpu\u001b[1;34m(token, numofword, cvmode, nsplit, n_candidate)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_mlflow_gpu\u001b[39m(\n\u001b[0;32m      2\u001b[0m             token: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,numofword: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      3\u001b[0m             cvmode: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKFold\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m             nsplit: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      5\u001b[0m             n_candidate: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      6\u001b[0m             ):\n\u001b[1;32m----> 8\u001b[0m     tfidf_arr\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatic_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtfidf_arr.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m     label_arr\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mload(static_path \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_arr.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m     label_names\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mload(static_path \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_names.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\numpy\\lib\\npyio.py:430\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    428\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode)\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 430\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\numpy\\lib\\format.py:756\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[0;32m    755\u001b[0m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[1;32m--> 756\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    758\u001b[0m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[0;32m    759\u001b[0m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    767\u001b[0m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[0;32m    768\u001b[0m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[0;32m    769\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mndarray(count, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.16 GiB for an array with shape (290543792,) and data type float64"
     ]
    }
   ],
   "source": [
    "crossvalid, result, result2 = mlFlow3(token=\"demo\",num_word=5,cvmode=\"KFold\",nsplit = 5,n_candidate = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0d170-d7d4-4851-9fae-df41ffae9652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
